# Values for default microservice projects

#-------------------------------#
# secret.yaml value section     #
#-------------------------------#
externalSecrets:
  # externalSecrets.enabled -- When set to true external secrets can be added and will create k8s secrets which will be injected into the container. If the container doesn't have any sensitive data this can be set to false.
  enabled: true
  service:
    # externalSecrets.service.key -- `Key` to AWS Secret Manager object where all sensitive data needed by your application should be stored. Each key in the Secret Manager Object should be named like your needed environment variable
    key:

#-------------------------------#
# deployment.yaml value section #
#-------------------------------#
deployment:
  spec:
    # deployment.spec.replicas -- The number of service instances to deploy.
    replicas: 1
    # deployment.spec.serviceAccountName -- The ServiceAccount this service will be associated with.
    serviceAccountName: default
    # deployment.spec.imagePullSecrets -- Image Pull Secret to access docker registry.
    imagePullSecrets:
    image:
      # deployment.spec.image.repository -- The docker repository to pull the service image from.
      repository:
      # deployment.spec.image.name -- The image name to use.
      name:
      # deployment.spec.image.tag -- The image version to use.
      tag: latest
      # deployment.spec.image.pullPolicy -- The default rule for downloading images.
      pullPolicy: Always
    containers:
      readinessProbe:
        httpGet:
          # deployment.spec.containers.readinessProbe.httpGet.path -- Service's http path on which to execute a readinessProbe
          path: /
          # deployment.spec.containers.readinessProbe.httpGet.port -- Service's http port on which to execute a readinessProbe
          port: 80
          # deployment.spec.containers.readinessProbe.httpGet.scheme -- Http Scheme to use for readinesProbe. Can be either `HTTP` or `HTTPS`.
          scheme: HTTP
        # deployment.spec.containers.readinessProbe.initialDelaySeconds -- Amount of time to wait before performing the first probe.
        initialDelaySeconds: 5
        # deployment.spec.containers.readinessProbe.periodSeconds -- How often to perform the probe (in seconds).
        periodSeconds: 10
        # deployment.spec.containers.readinessProbe.timeoutSeconds -- Number of seconds after which the probe times out.
        timeoutSeconds: 1
        # deployment.spec.containers.readinessProbe.successThreshold -- Threshold to be considered successful after having failed.
        successThreshold: 1
        # deployment.spec.containers.readinessProbe.failureThreshold -- Number of times to retry the probe before giving up.
        failureThreshold: 3
    resources:
      limits:
        # deployment.spec.resources.limits.cpu -- Total amount of CPU time that a container can use every 100 ms. See [Managing Compute Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) for a detailed description on resource usage.
        cpu: 200m
        # deployment.spec.resources.limits.memory -- The memory limit for a Pod. See [Managing Compute Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) for a detailed description on resource usage.
        memory: 235M
      requests:
        # deployment.spec.resources.requests.cpu -- Fractional amount of CPU allowed for a Pod. See [Managing Compute Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) for a detailed description on resource usage.
        cpu: 150m
        # deployment.spec.resources.requests.memory -- Amount of memory reserved for a Pod. See [Managing Compute Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) for a detailed description on resource usage.
        memory: 200M

#-------------------------------#
# service.yaml value section    #
#-------------------------------#
service:
  spec:
    # service.spec.type -- Specify what kind of service to deploy. See [Kubernetes Service Spec](https://kubernetes.io/docs/concepts/services-networking/service/) for details
    type: ClusterIP
    ports:
      http:
        # service.spec.ports.http.port -- The http port the service is exposed to in the cluster.
        port: 80
        # service.spec.ports.http.targetPort -- The http port the service listens to and to which requests will be sent.
        targetPort: 80
      https:
        # service.spec.ports.https.port -- The https port the service is exposed to in the cluster.
        port: 443
        # service.spec.ports.https.targetPort -- The http port the service listens to and to which requests will be sent.
        targetPort: 80

#--------------------------------#
# autoscaling.yaml value section #
#--------------------------------#
autoscaling:
  maxReplicas: 5
  minReplicas: 1
  metrics:
    resource:
      cpu:
        targetAverageUtilization: 80

#-------------------------------------#
# Extension param value section       #
#-------------------------------------#
additionalparameters:
  # additionalparameters.configMapApplied -- Set to `true` if you want to add a custom `ConfigMap` for your deployment.
  configMapApplied: false
  # additionalparameters.secretsApplied -- Set to `true` if you want to add a custom `Secret` for your deployment.
  secretsApplied: false
  # additionalparameters.yamlConfigFileApplied -- Set to `true` if you want to add a custom yaml configuration for your deployment.
  yamlConfigFileApplied: false
  config:
    PLACEHOLDER: "placeholder"
  secrets:
    PLACEHOLDER: "placeholder"
  yamlConfigFile:
    configFileProp:
      sub:
        value
    anotherConfigFileProp: 1

#-------------------------------------#
# Gloo (api gateway) value section    #
#-------------------------------------#
gloo:
  # gloo.enabled -- When set to true only the application's deployment resources will be installed with this chart. Can be used to explicitly avoid deploying a VirtualService resource.
  enabled: true
  virtualservice:
    # gloo.virtualservice.name -- Name of the `VirtualService` to deploy
    name:
    spec:
      sslConfig:
        secretRef:
          # gloo.virtualservice.spec.sslConfig.secretRef.name -- Name of the secret containing the certificate information for this deployment.
          name:
          # gloo.virtualservice.spec.sslConfig.secretRef.namespace -- Name of the namespace where the secret is located.
          namespace:
      virtualHost:
        # gloo.virtualservice.spec.virtualHost.domains -- List of DNS domain names this service will be published to.
        domains:
        routes:
          # gloo.virtualservice.spec.virtualHost.routes.callbackUrlPath -- Path to callback url which needs to be registered at the Identity Provider.
          callbackUrlPath: /callback
          upstream:
            # gloo.virtualservice.spec.virtualHost.routes.upstream.name -- Upstream k8s service to handle the incoming request after the authentication.
            name:
            # gloo.virtualservice.spec.virtualHost.routes.upstream.upstream.namespace -- Namespace where the upstream k8s service is located.
            namespace:
        # gloo.virtualservice.spec.virtualHost.routes.additionalRoutes -- List of route configurations for this `VirtualService`. See [gloo VirtualService Specification](https://docs.solo.io/gloo/1.1.0/introduction/concepts/#virtual-services) for details
          additionalRoutes:

  authConfig:
    # gloo.authConfig.name -- Name of the `Auth Config Plugin`
    name: auth-plugin
    # gloo.authConfig.namespace -- Namespace where the `Auth Config Plugin` is located
    namespace: default
    spec:
      configs:
        oauth:
          # gloo.authConfig.spec.configs.oauth.app_url -- `BaseUrl` of the app
          app_url: https://petstore.auth-sandbox.viper.bmw.cloud
          # gloo.authConfig.spec.configs.oauth.callback_path --  Registered `RedirectUrl` at the IDP
          callback_path: /callback
          # gloo.authConfig.spec.configs.oauth.client_id -- Registered `ClientID` at the IDP
          client_id: XXX
          client_secret_ref:
            # gloo.authConfig.spec.configs.oauth.client_secret_ref.name -- Name of the `Secret`. Gloo expects a k8s secret with the key `oauth`
            name: webeam-oidc
            # gloo.authConfig.spec.configs.oauth.client_secret_ref.namespace -- Namespace were the `Secret` is located
            namespace: default
          # gloo.authConfig.spec.configs.oauth.issuer_url -- Discovery URL to the Identity Provider
          issuer_url: https://auth-i.bmwgroup.net/auth/oauth2/realms/root/realms/intranetb2x/.well-known/openid-configuration
          # gloo.authConfig.spec.configs.oauth.scopes -- List of OIDC scopes. `openid` is set per default by Gloo and don't need to add here
          scopes:
        cachePlugin:
          # gloo.authConfig.spec.configs.cachePlugin.name -- `Name` of the cache plugin
          name: AuthFlow
          config:
            # gloo.authConfig.spec.configs.cachePlugin.config.CacheTableName -- `CacheTableName` of the auth cache
            CacheTableName: auth-cache-dev
            # gloo.authConfig.spec.configs.cachePlugin.config.AwsRegion -- `AwsRegion` where the cache is located
            AwsRegion: eu-west-1
        # gloo.authConfig.spec.configs.additionalPlugins -- List of plugins which should be added to the plugin chain. Expected format is a valid yaml with the `pluginAuth`. See [gloo Plugin Auth](https://docs.solo.io/gloo/latest/guides/security/auth/extauth/plugin_auth/#create-an-authconfig-resource) for details
        additionalPlugins:

#-------------------------------------#
# DataDog value section               #
#-------------------------------------#
datadog:
  # datadog.enabled -- When set to true Datadog is enabled and all logs, metrics and traces will be sent to Datadog.
  enabled: true
  # datadog.source -- Defines the source which creates log outputs. Source defines the log format and triggers Datadog parser pipelines
  source:
  # datadog.team -- Label in Datadog for the responsible team
  team:
